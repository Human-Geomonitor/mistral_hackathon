{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ea9de7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "     ------------------------------------- 211.1/211.1 kB 12.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from newspaper3k) (3.2.0)\n",
      "Collecting feedfinder2>=0.0.4\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from newspaper3k) (3.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.2)\n",
      "Collecting feedparser>=5.2.1\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "     ---------------------------------------- 81.3/81.3 kB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from newspaper3k) (4.9.1)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0)\n",
      "Collecting jieba3k>=0.35.1\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "     ---------------------------------------- 7.4/7.4 MB 16.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from newspaper3k) (2.28.1)\n",
      "Collecting tinysegmenter==0.3\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from newspaper3k) (9.2.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from newspaper3k) (4.11.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\priam\\anaconda3\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\priam\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\priam\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\priam\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.64.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\priam\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\priam\\anaconda3\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.5)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=5b4dc38ec8542041b3ea86d5af6bf6268a382ffb7e7a259dd834cdd4b8cfc3e9\n",
      "  Stored in directory: c:\\users\\priam\\appdata\\local\\pip\\cache\\wheels\\94\\ad\\df\\a2a01300cea47d5695f242f7e925a805970106fd9e4b151468\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=cede79a55997cb1d0287906382075cc8fec74be3415631d4229b54bdfb244aa6\n",
      "  Stored in directory: c:\\users\\priam\\appdata\\local\\pip\\cache\\wheels\\43\\4a\\c2\\61a371b2524ac90805391c660d8dc4505705297f25e2b85a5d\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398381 sha256=a9d267edbc441111ce9e58d2ec175af40d58215d058161ca81d71460676b4f3c\n",
      "  Stored in directory: c:\\users\\priam\\appdata\\local\\pip\\cache\\wheels\\c2\\22\\59\\8214a8d6357e9f540ce1f37f9a4362b6156b4ca81b37f1945f\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=c050b2e215d91fb0f398719330e82b3b90cd63e09c758b4910faa7a72f2356fc\n",
      "  Stored in directory: c:\\users\\priam\\appdata\\local\\pip\\cache\\wheels\\65\\7a\\a7\\78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, feedfinder2, newspaper3k\n",
      "Successfully installed feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 sgmllib3k-1.0.0 tinysegmenter-0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64ef5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import newspaper\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a1a3a",
   "metadata": {},
   "source": [
    "# Saving docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27874c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES = [\"Somalia\", \"Ethiopia\", \"Kenya\", \"Uganda\", \"South Sudan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5fe50e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "DATES = []\n",
    "\n",
    "def get_dates_between(start_date, end_date, interval_months):\n",
    "    dates_list = []\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        dates_list.append(current_date)\n",
    "        current_date += timedelta(days=30 * interval_months)\n",
    "\n",
    "    return dates_list\n",
    "\n",
    "start_date = datetime(2015, 1, 1)\n",
    "end_date = datetime(2024, 6, 26)\n",
    "interval_months = 6\n",
    "\n",
    "dates_between = get_dates_between(start_date, end_date, interval_months)\n",
    "for date in dates_between:\n",
    "    DATES.append(date.strftime(\"%Y-%m-%d\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e1c953a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateparser import parse\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0'\n",
    "\n",
    "config = Config()\n",
    "config.browser_user_agent = USER_AGENT\n",
    "config.request_timeout = 30\n",
    "\n",
    "def get_bbc_articles(country):\n",
    "    base_url = \"https://www.bbc.co.uk\"\n",
    "    search_url = f\"{base_url}/search?q={country}&seqId=a2619180-339d-11ef-b51b-05b0b4d9a140&d=NEWS_PS\"\n",
    "\n",
    "    articles = []\n",
    "    dates = []\n",
    "\n",
    "    # Iterate over search result pages\n",
    "    for page in range(1,35):  # Adjust range as needed to cover more pages\n",
    "        response = requests.get(search_url + f\"&page={page}\")\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Adjust the selector based on the actual structure\n",
    "        for article in soup.find_all('a', class_=\"ssrcss-its5xf-PromoLink exn3ah91\"):\n",
    "            base_url = article[\"href\"]\n",
    "            if not \"news\" in base_url:\n",
    "                continue\n",
    "            article = Article(base_url, config=config)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            soup_article = BeautifulSoup(article.html, 'html.parser')\n",
    "            bbc_dictionary = json.loads(\"\".join(soup_article.find(\"script\", {\"type\":\"application/ld+json\"}).contents))\n",
    "            date_published = [value for (key, value) in bbc_dictionary.items() if key == 'datePublished']\n",
    "            if len(date_published) > 0:\n",
    "                articles.append(\"Title: \" + article.title + \"\\n\" + article.text)\n",
    "                dates.append(datetime.strptime(date_published[0], '%Y-%m-%dT%H:%M:%S.%fZ').strftime('%Y-%m-%d'))\n",
    "    articles_df = pd.DataFrame(data={\"articles\": articles, \"dates\": dates})\n",
    "\n",
    "    return articles_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "53b7aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def write_doc(list_text, country, start_date, end_date):\n",
    "    split_list = [list_text[i:i + 10] for i in range(0, len(list_text), 10)]\n",
    "    for i in range(len(split_list)):\n",
    "        filename = \"_\".join([country, start_date, end_date, str(i)]) + \".txt\"\n",
    "        text_to_save = \"\\n\\n\\n\\n\\n\\n\\n\".join(split_list[i])\n",
    "        if text_to_save != \"\\n\\n\\n\\n\\n\\n\\n\":\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text_to_save)\n",
    "                print(f\"Saving {filename}\")\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e0c5f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_pipeline():\n",
    "    for country in COUNTRIES:\n",
    "        print(f\"Running for country {country}\")\n",
    "        articles_df = get_bbc_articles(country)\n",
    "        for i in range(len(DATES)-1):\n",
    "            start_date = DATES[i]\n",
    "            end_date = DATES[i+1]\n",
    "            print(f\"Running for {country} between {start_date} and {end_date}\")\n",
    "            subset_articles_df = articles_df.loc[(articles_df['dates'] > start_date) & (articles_df['dates'] <= end_date)]\n",
    "            if not subset_articles_df.empty:\n",
    "                texts = subset_articles_df[\"articles\"].to_list()\n",
    "                write_doc(texts, country, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "12c355f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for country Somalia\n",
      "Running for Somalia between 2015-01-01 and 2015-06-30\n",
      "Running for Somalia between 2015-06-30 and 2015-12-27\n",
      "Saving Somalia_2015-06-30_2015-12-27_0.txt\n",
      "Saving Somalia_2015-06-30_2015-12-27_1.txt\n",
      "Running for Somalia between 2015-12-27 and 2016-06-24\n",
      "Saving Somalia_2015-12-27_2016-06-24_0.txt\n",
      "Saving Somalia_2015-12-27_2016-06-24_1.txt\n",
      "Saving Somalia_2015-12-27_2016-06-24_2.txt\n",
      "Saving Somalia_2015-12-27_2016-06-24_3.txt\n",
      "Running for Somalia between 2016-06-24 and 2016-12-21\n",
      "Saving Somalia_2016-06-24_2016-12-21_0.txt\n",
      "Saving Somalia_2016-06-24_2016-12-21_1.txt\n",
      "Saving Somalia_2016-06-24_2016-12-21_2.txt\n",
      "Running for Somalia between 2016-12-21 and 2017-06-19\n",
      "Saving Somalia_2016-12-21_2017-06-19_0.txt\n",
      "Saving Somalia_2016-12-21_2017-06-19_1.txt\n",
      "Saving Somalia_2016-12-21_2017-06-19_2.txt\n",
      "Running for Somalia between 2017-06-19 and 2017-12-16\n",
      "Saving Somalia_2017-06-19_2017-12-16_0.txt\n",
      "Running for Somalia between 2017-12-16 and 2018-06-14\n",
      "Saving Somalia_2017-12-16_2018-06-14_0.txt\n",
      "Running for Somalia between 2018-06-14 and 2018-12-11\n",
      "Saving Somalia_2018-06-14_2018-12-11_0.txt\n",
      "Running for Somalia between 2018-12-11 and 2019-06-09\n",
      "Saving Somalia_2018-12-11_2019-06-09_0.txt\n",
      "Saving Somalia_2018-12-11_2019-06-09_1.txt\n",
      "Running for Somalia between 2019-06-09 and 2019-12-06\n",
      "Saving Somalia_2019-06-09_2019-12-06_0.txt\n",
      "Running for Somalia between 2019-12-06 and 2020-06-03\n",
      "Saving Somalia_2019-12-06_2020-06-03_0.txt\n",
      "Running for Somalia between 2020-06-03 and 2020-11-30\n",
      "Saving Somalia_2020-06-03_2020-11-30_0.txt\n",
      "Running for Somalia between 2020-11-30 and 2021-05-29\n",
      "Saving Somalia_2020-11-30_2021-05-29_0.txt\n",
      "Running for Somalia between 2021-05-29 and 2021-11-25\n",
      "Saving Somalia_2021-05-29_2021-11-25_0.txt\n",
      "Running for Somalia between 2021-11-25 and 2022-05-24\n",
      "Saving Somalia_2021-11-25_2022-05-24_0.txt\n",
      "Running for Somalia between 2022-05-24 and 2022-11-20\n",
      "Saving Somalia_2022-05-24_2022-11-20_0.txt\n",
      "Saving Somalia_2022-05-24_2022-11-20_1.txt\n",
      "Running for Somalia between 2022-11-20 and 2023-05-19\n",
      "Saving Somalia_2022-11-20_2023-05-19_0.txt\n",
      "Saving Somalia_2022-11-20_2023-05-19_1.txt\n",
      "Running for Somalia between 2023-05-19 and 2023-11-15\n",
      "Saving Somalia_2023-05-19_2023-11-15_0.txt\n",
      "Running for Somalia between 2023-11-15 and 2024-05-13\n",
      "Saving Somalia_2023-11-15_2024-05-13_0.txt\n",
      "Saving Somalia_2023-11-15_2024-05-13_1.txt\n",
      "Saving Somalia_2023-11-15_2024-05-13_2.txt\n",
      "Saving Somalia_2023-11-15_2024-05-13_3.txt\n",
      "Saving Somalia_2023-11-15_2024-05-13_4.txt\n",
      "Saving Somalia_2023-11-15_2024-05-13_5.txt\n",
      "Running for country Ethiopia\n",
      "Running for Ethiopia between 2015-01-01 and 2015-06-30\n",
      "Running for Ethiopia between 2015-06-30 and 2015-12-27\n",
      "Running for Ethiopia between 2015-12-27 and 2016-06-24\n",
      "Running for Ethiopia between 2016-06-24 and 2016-12-21\n",
      "Running for Ethiopia between 2016-12-21 and 2017-06-19\n",
      "Running for Ethiopia between 2017-06-19 and 2017-12-16\n",
      "Running for Ethiopia between 2017-12-16 and 2018-06-14\n",
      "Saving Ethiopia_2017-12-16_2018-06-14_0.txt\n",
      "Running for Ethiopia between 2018-06-14 and 2018-12-11\n",
      "Running for Ethiopia between 2018-12-11 and 2019-06-09\n",
      "Running for Ethiopia between 2019-06-09 and 2019-12-06\n",
      "Running for Ethiopia between 2019-12-06 and 2020-06-03\n",
      "Running for Ethiopia between 2020-06-03 and 2020-11-30\n",
      "Saving Ethiopia_2020-06-03_2020-11-30_0.txt\n",
      "Running for Ethiopia between 2020-11-30 and 2021-05-29\n",
      "Saving Ethiopia_2020-11-30_2021-05-29_0.txt\n",
      "Saving Ethiopia_2020-11-30_2021-05-29_1.txt\n",
      "Saving Ethiopia_2020-11-30_2021-05-29_2.txt\n",
      "Saving Ethiopia_2020-11-30_2021-05-29_3.txt\n",
      "Saving Ethiopia_2020-11-30_2021-05-29_4.txt\n",
      "Running for Ethiopia between 2021-05-29 and 2021-11-25\n",
      "Saving Ethiopia_2021-05-29_2021-11-25_0.txt\n",
      "Saving Ethiopia_2021-05-29_2021-11-25_1.txt\n",
      "Saving Ethiopia_2021-05-29_2021-11-25_2.txt\n",
      "Saving Ethiopia_2021-05-29_2021-11-25_3.txt\n",
      "Saving Ethiopia_2021-05-29_2021-11-25_4.txt\n",
      "Saving Ethiopia_2021-05-29_2021-11-25_5.txt\n",
      "Saving Ethiopia_2021-05-29_2021-11-25_6.txt\n",
      "Saving Ethiopia_2021-05-29_2021-11-25_7.txt\n",
      "Running for Ethiopia between 2021-11-25 and 2022-05-24\n",
      "Saving Ethiopia_2021-11-25_2022-05-24_0.txt\n",
      "Saving Ethiopia_2021-11-25_2022-05-24_1.txt\n",
      "Saving Ethiopia_2021-11-25_2022-05-24_2.txt\n",
      "Running for Ethiopia between 2022-05-24 and 2022-11-20\n",
      "Saving Ethiopia_2022-05-24_2022-11-20_0.txt\n",
      "Saving Ethiopia_2022-05-24_2022-11-20_1.txt\n",
      "Saving Ethiopia_2022-05-24_2022-11-20_2.txt\n",
      "Running for Ethiopia between 2022-11-20 and 2023-05-19\n",
      "Saving Ethiopia_2022-11-20_2023-05-19_0.txt\n",
      "Saving Ethiopia_2022-11-20_2023-05-19_1.txt\n",
      "Running for Ethiopia between 2023-05-19 and 2023-11-15\n",
      "Saving Ethiopia_2023-05-19_2023-11-15_0.txt\n",
      "Saving Ethiopia_2023-05-19_2023-11-15_1.txt\n",
      "Running for Ethiopia between 2023-11-15 and 2024-05-13\n",
      "Saving Ethiopia_2023-11-15_2024-05-13_0.txt\n",
      "Saving Ethiopia_2023-11-15_2024-05-13_1.txt\n",
      "Saving Ethiopia_2023-11-15_2024-05-13_2.txt\n",
      "Saving Ethiopia_2023-11-15_2024-05-13_3.txt\n",
      "Saving Ethiopia_2023-11-15_2024-05-13_4.txt\n",
      "Saving Ethiopia_2023-11-15_2024-05-13_5.txt\n",
      "Saving Ethiopia_2023-11-15_2024-05-13_6.txt\n",
      "Saving Ethiopia_2023-11-15_2024-05-13_7.txt\n",
      "Running for country Kenya\n",
      "Running for Kenya between 2015-01-01 and 2015-06-30\n",
      "Running for Kenya between 2015-06-30 and 2015-12-27\n",
      "Running for Kenya between 2015-12-27 and 2016-06-24\n",
      "Running for Kenya between 2016-06-24 and 2016-12-21\n",
      "Running for Kenya between 2016-12-21 and 2017-06-19\n",
      "Running for Kenya between 2017-06-19 and 2017-12-16\n",
      "Running for Kenya between 2017-12-16 and 2018-06-14\n",
      "Running for Kenya between 2018-06-14 and 2018-12-11\n",
      "Running for Kenya between 2018-12-11 and 2019-06-09\n",
      "Running for Kenya between 2019-06-09 and 2019-12-06\n",
      "Running for Kenya between 2019-12-06 and 2020-06-03\n",
      "Running for Kenya between 2020-06-03 and 2020-11-30\n",
      "Running for Kenya between 2020-11-30 and 2021-05-29\n",
      "Running for Kenya between 2021-05-29 and 2021-11-25\n",
      "Running for Kenya between 2021-11-25 and 2022-05-24\n",
      "Running for Kenya between 2022-05-24 and 2022-11-20\n",
      "Saving Kenya_2022-05-24_2022-11-20_0.txt\n",
      "Saving Kenya_2022-05-24_2022-11-20_1.txt\n",
      "Saving Kenya_2022-05-24_2022-11-20_2.txt\n",
      "Saving Kenya_2022-05-24_2022-11-20_3.txt\n",
      "Saving Kenya_2022-05-24_2022-11-20_4.txt\n",
      "Running for Kenya between 2022-11-20 and 2023-05-19\n",
      "Saving Kenya_2022-11-20_2023-05-19_0.txt\n",
      "Saving Kenya_2022-11-20_2023-05-19_1.txt\n",
      "Saving Kenya_2022-11-20_2023-05-19_2.txt\n",
      "Saving Kenya_2022-11-20_2023-05-19_3.txt\n",
      "Running for Kenya between 2023-05-19 and 2023-11-15\n",
      "Saving Kenya_2023-05-19_2023-11-15_0.txt\n",
      "Saving Kenya_2023-05-19_2023-11-15_1.txt\n",
      "Saving Kenya_2023-05-19_2023-11-15_2.txt\n",
      "Saving Kenya_2023-05-19_2023-11-15_3.txt\n",
      "Saving Kenya_2023-05-19_2023-11-15_4.txt\n",
      "Running for Kenya between 2023-11-15 and 2024-05-13\n",
      "Saving Kenya_2023-11-15_2024-05-13_0.txt\n",
      "Saving Kenya_2023-11-15_2024-05-13_1.txt\n",
      "Saving Kenya_2023-11-15_2024-05-13_2.txt\n",
      "Saving Kenya_2023-11-15_2024-05-13_3.txt\n",
      "Saving Kenya_2023-11-15_2024-05-13_4.txt\n",
      "Saving Kenya_2023-11-15_2024-05-13_5.txt\n",
      "Saving Kenya_2023-11-15_2024-05-13_6.txt\n",
      "Saving Kenya_2023-11-15_2024-05-13_7.txt\n",
      "Running for country Uganda\n",
      "Running for Uganda between 2015-01-01 and 2015-06-30\n",
      "Running for Uganda between 2015-06-30 and 2015-12-27\n",
      "Running for Uganda between 2015-12-27 and 2016-06-24\n",
      "Running for Uganda between 2016-06-24 and 2016-12-21\n",
      "Running for Uganda between 2016-12-21 and 2017-06-19\n",
      "Running for Uganda between 2017-06-19 and 2017-12-16\n",
      "Running for Uganda between 2017-12-16 and 2018-06-14\n",
      "Running for Uganda between 2018-06-14 and 2018-12-11\n",
      "Saving Uganda_2018-06-14_2018-12-11_0.txt\n",
      "Saving Uganda_2018-06-14_2018-12-11_1.txt\n",
      "Running for Uganda between 2018-12-11 and 2019-06-09\n",
      "Saving Uganda_2018-12-11_2019-06-09_0.txt\n",
      "Saving Uganda_2018-12-11_2019-06-09_1.txt\n",
      "Running for Uganda between 2019-06-09 and 2019-12-06\n",
      "Saving Uganda_2019-06-09_2019-12-06_0.txt\n",
      "Saving Uganda_2019-06-09_2019-12-06_1.txt\n",
      "Running for Uganda between 2019-12-06 and 2020-06-03\n",
      "Saving Uganda_2019-12-06_2020-06-03_0.txt\n",
      "Saving Uganda_2019-12-06_2020-06-03_1.txt\n",
      "Running for Uganda between 2020-06-03 and 2020-11-30\n",
      "Saving Uganda_2020-06-03_2020-11-30_0.txt\n",
      "Saving Uganda_2020-06-03_2020-11-30_1.txt\n",
      "Running for Uganda between 2020-11-30 and 2021-05-29\n",
      "Saving Uganda_2020-11-30_2021-05-29_0.txt\n",
      "Saving Uganda_2020-11-30_2021-05-29_1.txt\n",
      "Saving Uganda_2020-11-30_2021-05-29_2.txt\n",
      "Running for Uganda between 2021-05-29 and 2021-11-25\n",
      "Saving Uganda_2021-05-29_2021-11-25_0.txt\n",
      "Saving Uganda_2021-05-29_2021-11-25_1.txt\n",
      "Running for Uganda between 2021-11-25 and 2022-05-24\n",
      "Saving Uganda_2021-11-25_2022-05-24_0.txt\n",
      "Saving Uganda_2021-11-25_2022-05-24_1.txt\n",
      "Running for Uganda between 2022-05-24 and 2022-11-20\n",
      "Saving Uganda_2022-05-24_2022-11-20_0.txt\n",
      "Saving Uganda_2022-05-24_2022-11-20_1.txt\n",
      "Saving Uganda_2022-05-24_2022-11-20_2.txt\n",
      "Running for Uganda between 2022-11-20 and 2023-05-19\n",
      "Saving Uganda_2022-11-20_2023-05-19_0.txt\n",
      "Saving Uganda_2022-11-20_2023-05-19_1.txt\n",
      "Running for Uganda between 2023-05-19 and 2023-11-15\n",
      "Saving Uganda_2023-05-19_2023-11-15_0.txt\n",
      "Saving Uganda_2023-05-19_2023-11-15_1.txt\n",
      "Saving Uganda_2023-05-19_2023-11-15_2.txt\n",
      "Running for Uganda between 2023-11-15 and 2024-05-13\n",
      "Saving Uganda_2023-11-15_2024-05-13_0.txt\n",
      "Saving Uganda_2023-11-15_2024-05-13_1.txt\n",
      "Saving Uganda_2023-11-15_2024-05-13_2.txt\n",
      "Saving Uganda_2023-11-15_2024-05-13_3.txt\n",
      "Saving Uganda_2023-11-15_2024-05-13_4.txt\n",
      "Saving Uganda_2023-11-15_2024-05-13_5.txt\n",
      "Running for country South Sudan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for South Sudan between 2015-01-01 and 2015-06-30\n",
      "Saving South Sudan_2015-01-01_2015-06-30_0.txt\n",
      "Saving South Sudan_2015-01-01_2015-06-30_1.txt\n",
      "Running for South Sudan between 2015-06-30 and 2015-12-27\n",
      "Saving South Sudan_2015-06-30_2015-12-27_0.txt\n",
      "Saving South Sudan_2015-06-30_2015-12-27_1.txt\n",
      "Saving South Sudan_2015-06-30_2015-12-27_2.txt\n",
      "Saving South Sudan_2015-06-30_2015-12-27_3.txt\n",
      "Running for South Sudan between 2015-12-27 and 2016-06-24\n",
      "Saving South Sudan_2015-12-27_2016-06-24_0.txt\n",
      "Saving South Sudan_2015-12-27_2016-06-24_1.txt\n",
      "Running for South Sudan between 2016-06-24 and 2016-12-21\n",
      "Saving South Sudan_2016-06-24_2016-12-21_0.txt\n",
      "Saving South Sudan_2016-06-24_2016-12-21_1.txt\n",
      "Saving South Sudan_2016-06-24_2016-12-21_2.txt\n",
      "Saving South Sudan_2016-06-24_2016-12-21_3.txt\n",
      "Running for South Sudan between 2016-12-21 and 2017-06-19\n",
      "Saving South Sudan_2016-12-21_2017-06-19_0.txt\n",
      "Saving South Sudan_2016-12-21_2017-06-19_1.txt\n",
      "Running for South Sudan between 2017-06-19 and 2017-12-16\n",
      "Saving South Sudan_2017-06-19_2017-12-16_0.txt\n",
      "Running for South Sudan between 2017-12-16 and 2018-06-14\n",
      "Saving South Sudan_2017-12-16_2018-06-14_0.txt\n",
      "Running for South Sudan between 2018-06-14 and 2018-12-11\n",
      "Saving South Sudan_2018-06-14_2018-12-11_0.txt\n",
      "Running for South Sudan between 2018-12-11 and 2019-06-09\n",
      "Saving South Sudan_2018-12-11_2019-06-09_0.txt\n",
      "Running for South Sudan between 2019-06-09 and 2019-12-06\n",
      "Saving South Sudan_2019-06-09_2019-12-06_0.txt\n",
      "Running for South Sudan between 2019-12-06 and 2020-06-03\n",
      "Saving South Sudan_2019-12-06_2020-06-03_0.txt\n",
      "Running for South Sudan between 2020-06-03 and 2020-11-30\n",
      "Running for South Sudan between 2020-11-30 and 2021-05-29\n",
      "Saving South Sudan_2020-11-30_2021-05-29_0.txt\n",
      "Running for South Sudan between 2021-05-29 and 2021-11-25\n",
      "Saving South Sudan_2021-05-29_2021-11-25_0.txt\n",
      "Running for South Sudan between 2021-11-25 and 2022-05-24\n",
      "Saving South Sudan_2021-11-25_2022-05-24_0.txt\n",
      "Running for South Sudan between 2022-05-24 and 2022-11-20\n",
      "Saving South Sudan_2022-05-24_2022-11-20_0.txt\n",
      "Running for South Sudan between 2022-11-20 and 2023-05-19\n",
      "Saving South Sudan_2022-11-20_2023-05-19_0.txt\n",
      "Saving South Sudan_2022-11-20_2023-05-19_1.txt\n",
      "Running for South Sudan between 2023-05-19 and 2023-11-15\n",
      "Running for South Sudan between 2023-11-15 and 2024-05-13\n",
      "Saving South Sudan_2023-11-15_2024-05-13_0.txt\n",
      "Saving South Sudan_2023-11-15_2024-05-13_1.txt\n"
     ]
    }
   ],
   "source": [
    "doc_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "70d5f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = get_bbc_articles(\"Somalia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "10fd0ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Somalia between 2022-05-24 and 2022-11-20\n",
      "Saving Somalia_2022-05-24_2022-11-20_0.txt\n",
      "Saving Somalia_2022-05-24_2022-11-20_1.txt\n"
     ]
    }
   ],
   "source": [
    "start_date = \"2022-05-24\"\n",
    "end_date = \"2022-11-20\"\n",
    "country = \"Somalia\"\n",
    "print(f\"Running for {country} between {start_date} and {end_date}\")\n",
    "subset_articles_df = articles_df.loc[(articles_df['dates'] > start_date) & (articles_df['dates'] <= end_date)]\n",
    "if not subset_articles_df.empty:\n",
    "    list_text = subset_articles_df[\"articles\"].to_list()\n",
    "split_list = [list_text[i:i + 10] for i in range(0, len(list_text), 10)]\n",
    "for i in range(len(split_list)):\n",
    "    filename = \"_\".join([country, start_date, end_date, str(i)]) + \".txt\"\n",
    "    text_to_save = \"\\n\\n\\n\\n\\n\\n\\n\".join(split_list[i])\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text_to_save)\n",
    "        print(f\"Saving {filename}\")\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
