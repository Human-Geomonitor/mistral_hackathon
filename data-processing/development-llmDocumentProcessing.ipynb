{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM document processing\n",
    "\n",
    "This notebook aims at testing the delpoyment of LLMs and use them to process documents for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import the right version of torch with cuda unabled:\n",
    "\n",
    "```pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121```\n",
    "\n",
    "The other modules can be imported with ```pip install -r Pipeline/env/requirements.txt```.\n",
    "\n",
    "Be carefull to run this command in the right virtual envorionment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_12152\\1635270516.py\", line 6, in <module>\n",
      "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True, token = token)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 562, in from_pretrained\n",
      "    model_class = _get_model_class(config, cls._model_mapping)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 383, in _get_model_class\n",
      "    supported_models = model_mapping[type(config)]\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 734, in __getitem__\n",
      "    return self._load_attr_from_module(model_type, model_name)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 748, in _load_attr_from_module\n",
      "    return getattribute_from_module(self._modules[module_name], attr)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 692, in getattribute_from_module\n",
      "    if hasattr(module, attr):\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1525, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1535, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py\", line 35, in <module>\n",
      "    from ...modeling_utils import PreTrainedModel\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\modeling_utils.py\", line 45, in <module>\n",
      "    from .generation import GenerationConfig, GenerationMixin\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1525, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1535, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\generation\\utils.py\", line 97, in <module>\n",
      "    from accelerate.hooks import AlignDevicesHook, add_hook_to_module\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\accelerate\\__init__.py\", line 16, in <module>\n",
      "    from .accelerator import Accelerator\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\accelerate\\accelerator.py\", line 35, in <module>\n",
      "    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\accelerate\\checkpointing.py\", line 24, in <module>\n",
      "    from .utils import (\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\accelerate\\utils\\__init__.py\", line 183, in <module>\n",
      "    from .fsdp_utils import load_fsdp_model, load_fsdp_optimizer, merge_fsdp_weights, save_fsdp_model, save_fsdp_optimizer\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\accelerate\\utils\\fsdp_utils.py\", line 29, in <module>\n",
      "    import torch.distributed.checkpoint as dist_cp\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\distributed\\checkpoint\\__init__.py\", line 2, in <module>\n",
      "    from .default_planner import DefaultLoadPlanner, DefaultSavePlanner\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\distributed\\checkpoint\\default_planner.py\", line 13, in <module>\n",
      "    from torch.distributed._tensor import DTensor\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\distributed\\_tensor\\__init__.py\", line 6, in <module>\n",
      "    import torch.distributed._tensor.ops\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\distributed\\_tensor\\ops\\__init__.py\", line 2, in <module>\n",
      "    from .embedding_ops import *  # noqa: F403\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\distributed\\_tensor\\ops\\embedding_ops.py\", line 8, in <module>\n",
      "    import torch.distributed._functional_collectives as funcol\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\distributed\\_functional_collectives.py\", line 12, in <module>\n",
      "    from . import _functional_collectives_impl as fun_col_impl\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\distributed\\_functional_collectives_impl.py\", line 36, in <module>\n",
      "    from torch._dynamo import assume_constant_result\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\_dynamo\\__init__.py\", line 64, in <module>\n",
      "    torch.manual_seed = disable(torch.manual_seed)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\_dynamo\\decorators.py\", line 50, in disable\n",
      "    return DisableContext()(fn)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 410, in __call__\n",
      "    (filename is None or trace_rules.check(fn))\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3378, in check\n",
      "    return check_verbose(obj, is_inlined_call).skipped\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3361, in check_verbose\n",
      "    rule = torch._dynamo.trace_rules.lookup_inner(\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 3442, in lookup_inner\n",
      "    rule = get_torch_obj_rule_map().get(obj, None)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2782, in get_torch_obj_rule_map\n",
      "    obj = load_object(k)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2811, in load_object\n",
      "    val = _load_obj_from_str(x[0])\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py\", line 2795, in _load_obj_from_str\n",
      "    return getattr(importlib.import_module(module), obj_name)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py\", line 417, in <module>\n",
      "    values=torch.randn(3, 3, device=\"meta\"),\n",
      "c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py:417: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  values=torch.randn(3, 3, device=\"meta\"),\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\David\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 3/3 [16:43<00:00, 334.34s/it]\n",
      "c:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\accelerate\\utils\\modeling.py:1384: UserWarning: Current model requires 41943360.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\modeling_utils.py:3703\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3700\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   3702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3703\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3705\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3706\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Users\\David\\miniconda3\\envs\\llm-pipeline\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:85\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m     device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     82\u001b[0m         key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_not_convert\n\u001b[0;32m     83\u001b[0m     }\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m---> 85\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         )\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True, token = token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
